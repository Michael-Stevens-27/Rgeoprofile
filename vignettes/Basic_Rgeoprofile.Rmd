---
title: "Basic Rgeoprofile"
author: "Michael Stevens"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Basic Rgeoprofile}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Introduction

In this tutorial, we will walk through the step-by-step process of running an analysis using the R package, RgeoProfile. The general structure to using this package is as follows:

* Loading and checking our data - we must ensure it's in the correct format for analysis. 
* Setting up the parameters for the model and the MCMC algorithm.
* Running the MCMC algorithm. 
* Visualising the output of the model, for example the geoprofile.   

First things first, we must load the package. Assuming the RgeoProfile package was [installed correctly](https://michael-stevens-27.github.io/Rgeoprofile/articles/installation.html), we can load in the package using `library(RgeoProfile)`. In addition to this we will be using the leaflet and raster packages for creating maps.  

```{r, eval = TRUE, message = FALSE}
# load prequired packages
library(RgeoProfile)  # to run the analysis
library(leaflet)      # to create interactive maps
library(raster)       # for general spatial analysis
```

## Load and check data

Next, we need to load in some data for the model to analyse. The package comes with some pre-made data that can be accessed via the `LondonExample_crimes` and `LondonExample_sources` objects. These consist of a set of longitudinal/latitudinal points and the source locations (also in long/lat format) that are responsible for said crimes. Normally we would read in our data using the `read.table` or `read.csv` functions but since the example data is provided to us, we will use this instead. We will also process the data using the `geoData()` and `geoDataSource()` functions to ensure that the data are in the correct format for running the model.    
 
```{r, eval = TRUE, message = FALSE}
# load the data 
data(LondonExample_crimes)
data(LondonExample_sources)

# example data
d <- LondonExample_crimes
s <- LondonExample_sources

# convert d and s to correct format for geoParams()
# (note that in this case the example data are already in the correct
# format; these steps are only relevant if for example d and s are 
# imported as two-column matrices. They are included here for
# completeness)

d <- geoData(d$longitude, d$latitude)
s <- geoDataSource(s$longitude, s$latitude)

print(d)
print(s)
```

Here we can see that we've got a set of 50 long/lat points and 4 source locations. 

## Build a parameters objects

Next we need to set the parameters that will govern the model that we are running. We do this using the `geoParams()` function. Firstly we feed this function our data object `d` using the `data` argument. Next we define our prior on the dispersal parameter sigma. This can be deinfed in a few different ways but we will stick with expressing out prior knowledge on sigma by stating our mean (`sigma_mean`) and variance (`sigma_var`) on it. Next we specify the MCMC parameters for the model. We set the length of the burn in (`burnin`) period and the number of sampling iterations (`samples`) as well as the number of MCMC chains to run in parallel (`chains`). Finally we set some parameters for controlling spatial extent and the resolution of the geoprofile. This is done via the `longitude_cells`, `latitude_cells` and `guardRail` arguments. To customise the priors and MCMC algorithm further, see the `geoParams()` documentation for the full list of arguments and defaults.  

```{r, eval = TRUE}
# set model and MCMC parameters
p <- geoParams(data = d, 
               sigma_mean = 1, 
               sigma_var = 5, 
               chains = 5, 
               burnin = 1e3, 
               samples = 1e4,
               longitude_cells = 200, 
               latitude_cells = 200, 
               guardRail = 0.1)

print(p)
```

Here we can see the full list of model and MCMC parameters that we will be using. Now that we have set up a parameters object we can now run the model. 

## Run the model

Running the MCMC algorithm itself is straightforward. We run the model using the `geoMCMC()` function. We provide this function with our `data` and our `params`. 

```{r, eval = TRUE}

# run MCMC
m <- geoMCMC(data = d, params = p)
```

Here we can see the model stating how far through the burn in and sampling phases it is. Notice the burn in phase states that the five MCMC chains have all converged to the same answer within 100 iterations using the Gelamn-Rubin diagnostic (REF). The sampling phase then ends with a "maximum likelihood lambda". This essentially states the bandwidth of the kernel density estimator to create the geoprofile (REF). 
    
## Visualise output

The top parameter of interest that we are trying to estimate are the source locations responsible for generating the data. So first we want to have a look at our geographic profile. We generate the geographic profile using the `geoplotmap2()` function. The main things we must provide to this function are the params and the raw geoprofile produced by the model. Hence we provide these using the `params` and `surface` arguments. 

```{r, eval = TRUE}
# plot profile on map
geoPlotMap2(params = p,
            surface = m$geoProfile)
```

Here the colour scheme moves from yellow to purple, where yellow indicates areas we believe might conatin a source location. The purple on the other hand illustrates areas we believe are less likely to contain a source location. This does not tell us too much. The `geoplotmap2()` function can be customised to display more information on the map. Let's add sources and crimes onto the map, as well as a legend that describes the colours we are seeing. 

```{r, eval = TRUE}  
geoPlotMap2(surface = m$geoProfile,
            params = p,
            data = d, 
            source = s, 
            surfaceCols = rev(viridis::plasma(10)),
            opacity = 0.7,
            threshold = 0.1,
            crimeCex = 2,
            sourceCex = 3,
            sourceCol = "black",
            gpLegend = TRUE)                       
```

Much better! So now we can see the explicit source and crime locations on the map. The map is only showing the top 10\% of the geoprofile. This is indicated by the legend too. This can be quickly changed using the `threshold` argument. Let's change this so we're looking at the top 50\% instead.

```{r, eval = TRUE}  
geoPlotMap2(surface = m$geoProfile,
            params = p,
            data = d, 
            source = s, 
            surfaceCols = rev(viridis::plasma(10)),
            opacity = 0.7,
            threshold = 0.5, # how top 50% of the geoprofile
            crimeCex = 2,
            sourceCex = 3,
            sourceCol = "black",
            gpLegend = TRUE)                       
```
 
Our model seems to be doing well, since the source locations (in black) are all landing in yellow areas. Looking at the map gives us a quick indication of how well our model is doing but how do we explicitly quantify model success? This is done via the source's *hit score* percentages and overall *Gini co-efficent*. Recall that the hit score percentage is the percentage of the area searched before finding the source locations divided by the total area searched. So the lower the hit score, the better the model is performing. Each source comes with its own hit score, but we can summarise these values using the Gini co-efficent. If we possessed the perfect model that found our source locations by searching almost none of the search area then we would expect a Gini co-efficent of one and if the model fails to find source locations after searching the entire search area, then we would expect a Gini value of zero. If we were to search for our source randomly then we would expect a Gini co-efficient of 0.5.

Hit scores are produced via the `geoReportHitscores()` function where we provide the parameters, the sources and the geoprofile. We can then obtain a Gini co-efficent using these hit scores (see the `geoPlotLorenz()` function).

```{r, eval = TRUE}

# get hitscores
hs <- geoReportHitscores(params = p, 
                         source = s, 
                         surface = m$geoProfile)
hs

# produce Lorenz plot
Gini <- geoPlotLorenz(hit_scores = hs)
Gini
```

We can see that the model found every source by searching less than 4\% of the total search area, excellent! Equivalently, the Gini co-efficient for these sources is 0.976. This tutorial was a quick walkthrough of the process one follows when running a geographic profiling analysis. The next tutorial will walkthrough analysing some real-world data in the form of inferring the home and work place of an individual via where their credit card has been used.  

```{r, echo = FALSE, eval = FALSE}
# save objects for later loading
# saveRDS(m, file =  "/home/mstevens/Desktop/MAIN WORK/Bangor Workshop/Rgeoprofile/inst/extdata/mcmc_object.rds")
# saveRDS(p, file =  "/home/mstevens/Desktop/MAIN WORK/Bangor Workshop/Rgeoprofile/inst/extdata/params_object.rds")
```
